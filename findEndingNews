
	

#import urllib.request
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
import time
from bs4 import BeautifulSoup
import re
import csv

month_names = ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December']	
#Next, the instance of Firefox WebDriver is created.
driver = webdriver.Firefox()


def find_Open_Ct (info) :
	#month_names already defined
	startCt = 0
	for line in info:
		startCt = startCt + 1
		if line.name == "span":
			for m in month_names:
				if m in line.text: 
					return startCt -1

def find_End_Ct (info) :
	#Ending looks like this: b'<span>AdChoices</span>' or like this: '<span id="yfs_params_vcr" style="display:none">{"yrb_token" : "YFT_MARKET_CLOSED", "tt" : "1380231285", "s" : "qcor", "k" : "a'
	EndCt = 0
	for line in info:
		EndCt = EndCt + 1
		if line.name == "span" and 'AdChoices' in line.text:
			return EndCt - 1
		if line.name == "span" and 'yrb_token' in line.text:
			return EndCt - 1

def parseDate(str):
	list = []
	out = []
	for i in range(len(str)):
		if ',' == str[i]:
			list.append(i)
	comma1 = list[0]
	comma2 = list[1]
	DayName = str[:comma1]	
	nameAndMonth = str[comma1+2:comma2]
	spaceLoc = nameAndMonth.find(' ')
	Month = nameAndMonth[:spaceLoc]
	DayNum = nameAndMonth[spaceLoc+1:]
	Year = str[comma2+2:]

	out = [DayName,Month,DayNum,Year]
	return out					

driver.get("http://finance.yahoo.com/q/h?s=QCOR&t=2012-05-14T10:29:59-04:00&id=54081667")
elem = driver.find_element_by_id("txtQuotes")
elem.send_keys("QCOR")
elem.send_keys(Keys.RETURN)
time.sleep(1.5)



def click_get_older_news():
	driver.find_element_by_partial_link_text("Older Headlines").click()
	time.sleep(1.5)

def create_text_file_name(ticker):
	fileName = 'YahooHeadlines_' + ticker + '.csv'
	return fileName


headline = "1" #default 1 to ensure we enter the loop
#while (headline != ""): #<====================#===========#=#=#=#=#===============!!!!!!!!!


fileName = create_text_file_name("QCOR")


soup=BeautifulSoup(driver.page_source)
val = 0
startCt = 0
endCt = 0

with open(fileName, 'a', newline='') as csvfile:
	#writer = csv.writer(csvfile, delimiter=',', quotechar='"', quoting=csv.QUOTE_NONE)
	writer = csv.writer(csvfile, delimiter=',')

	info = soup.find_all(text = True)

	startCt = find_Open_Ct(info)
	endCt = find_End_Ct (info)

	currDateStamp = ''
	headline = ''
	link = ''
	newCitation = ''

	print(startCt)
	print(endCt)


	for item in info[startCt:endCt]:
		print('--->', item.text) #$$$$$$$$

		if item.name == "span":
			for m in month_names:
				if m in item.text:
					currDateStamp = item.text
					currDateStampArray = parseDate(currDateStamp) #<----------------------------
					currDateStamp = item.text.encode('utf-8')
					#print('----->',currDateStamp)

					#print(currDateStamp)
		if item.name == "li" and (" Mail" not in item.text):
			#for citation
			citation = item.find('cite').text
			parens = citation.find('(')
			newCitation = citation[0:parens-1] #extra one space back to remove encoding error

			#removing 'at' from 'at Wall St. Cheat Sheet' if 'at' is present
			if 'at ' in newCitation:
				newCitation = newCitation[3:]
		
			#encode newCitation after manipulation in teh if statement
			newCitation = newCitation.encode('utf-8') #<------------
		

			#get headline name
			headline = item.find('a').text.encode('utf',errors='ignore') #<-----------
		
			#get link
			link = item.find('a').get('href').encode('utf-8') #<------
		

		if currDateStamp and headline and link and newCitation and currDateStampArray:
			newsLine = [currDateStampArray[0], currDateStampArray[1], currDateStampArray[2], currDateStampArray[3], 'QCOR', headline.decode("utf-8"), newCitation.decode("utf-8"), link.decode("utf-8")]
			
			lastHeadline = headline #used to check if end of news history
			writer.writerow( newsLine )#<-----------------------------------------------------------------------------------------------------------------------------
			#print(newsLine)
			link = ''
			newCitation = ''


	#click_get_older_news()	




#--------------------------------READING WRITING CSV----------------------------------------
 
#with open('test.csv', 'w', newline='') as csvfile:
	#writer = csv.writer(ofile, delimiter=',', quotechar='"', quoting=csv.QUOTE_NONE)
 	#writer.writerow(  .. )
#for row in reader:
    #writer.writerow(row)

#-------------------------------------------------------------------------------------------

#b'<span>Wednesday, September 25, 2013</span>'
#Home
#http://us.lrd.yahoo.com/_ylt=AvH.CdwQX6U5rypAMylCiMGuv7gF/SIG=119r32ntr/EXP=1381429544/**http%3A//www.yahoo.com/


#for line in soup.find_all(['span', 'li']):
	#startCt = startCt + 1
	#print(line.getText().encode('utf-8'))
	#if line.name == "span":
		#for m in month_names:
			#if m in line.text:	
				#val = 5
	#if line.name == "li":
		#val = 2

	#print(val,line.encode('utf-8'))
	#print(val, line.encode('utf-8'))


	#print(spans.get('attr'))





	#strs = spans.findAll(text=True)
	#for s in strs:
		#for monthNames in month_names:
			#if monthNames in s:
				#print(s)




#for spans in soup.find_all('span', text = "September"):
	#for monthName in month_names:
		#if monthName in spans:
			#print(spans.encode('utf-8'))


#in soup.find_all('span')
#soupW.find_all(["a", "b"])


#	p = spans.parent

#b'<span>Wednesday, September 11, 2013</span>' basically, if the span contains a (full name of weekdate like Wednesday), then a Month name, then a 4 digit #. This is a sign that the tag is what you are looking for

#b'<span>(Wed, Sep 11)</span>'


#print(soup.prettify())

#for link in soup.find_all('li'):
    #print(link)

#A function -> used to define a function, see the website http://www.crummy.com/software/BeautifulSoup/bs4/doc/#a-regular-expression

#soup.find_all('a')
#print soup.encode('utf-8')
#print(soup.prettify())
#print(soup.find_all("a"))
#print(soup.get_text())

#for link in soup.find_all('li'):
    #print(link.get('href'))